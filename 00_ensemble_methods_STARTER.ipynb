{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding it Forward: Machine Learning & The Wisdom of the Crowds\n",
    "\n",
    "<img src=\"./images/CIF_wordmark.png\">\n",
    "\n",
    "By the end of this notebook, you'll be able to fit ensemble models of both the bagging and boosting variety. \n",
    "\n",
    "This notebook is organized into four main sections:\n",
    "1. Read in the data and data prep.\n",
    "2. Fit and visualize a single decision tree with `scikit-learn`.\n",
    "3. Motivation behind ensemble modeling.\n",
    "4. Fit ensemble models with `scikit-learn`.\n",
    "    - Fit bagging models.\n",
    "    - Fit boosting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we'll read in all of our libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, \\\n",
    "ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "\n",
    "# Set a random seed to ensure reproducibility. These\n",
    "# algorithms involve randomness, so setting a seed is\n",
    "# helpful for us to ensure we get similar results!\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Read in the data and data prep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used here examines 462 individuals (all male) from South Africa, whether or not they were diagnosed with coronary heart disease (CHD), and a number of covariates. This dataset was shared via a 1983 South African Medical Journal article published by Rousseauw _et al_. and was downloaded from [the _Elements of Statistical Learning_ dataset page](https://web.stanford.edu/~hastie/ElemStatLearn/data.html) hosted by Trevor Hastie. (Adapted from [this page](https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.info.txt).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read our data into Pandas.\n",
    "data = pd.read_csv('./SAheart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine first five rows of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from [this page](https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.info.txt).\n",
    "- `sbp`: systolic blood pressure\n",
    "- `tobacco`: cumulative tobacco (kg)\n",
    "- `ldl`: low density lipoprotein cholesterol\n",
    "- `adiposity`: _no description given_\n",
    "- `famhist`: family history of heart disease (Present, Absent)\n",
    "- `typea`: type-A behavior\n",
    "- `obesity`: _no description given_\n",
    "- `alcohol`: current alcohol consumption\n",
    "- `age`: age at onset\n",
    "- `chd`: response variable, 1 = coronary heart disease, 0 = no CHD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit our models, we'll need to convert the `famhist` variable to numeric. I'll convert values of `Present` to 1 and `Absent` to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map 'Present' to 1 and 'Absent' to 0 and\n",
    "# overwrite the original 'famhist' column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming we converted to 1s and 0s.\n",
    "data['famhist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our X (inputs) and y (output) variables.\n",
    "X =\n",
    "y ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Fit and visualize a single decision tree with `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, I select a max_depth of 2. This \n",
    "# means that our tree will only grow to include 2\n",
    "# levels below the initial \"root\" node.\n",
    "\n",
    "# Since I want to visualize our output below, a\n",
    "# max_depth of 2 is a convenient decision. However,\n",
    "# you can play around with many different values!\n",
    "\n",
    "single_tree = DecisionTreeClassifier(max_depth=2,\n",
    "                                     random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit one single decision tree (CART) to our.\n",
    "# training data.\n",
    "single_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize plot.\n",
    "plt.figure(figsize = (24,10))\n",
    "\n",
    "# Plot visual representation of our decision tree.\n",
    "plot_tree(single_tree,\n",
    "          feature_names = X_train.columns,\n",
    "          class_names = ['No CHD', 'CHD'],\n",
    "          filled = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model predictions from our training and testing data.\n",
    "y_train_preds = single_tree.predict(X_train)\n",
    "y_test_preds = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our model's performance using the \n",
    "# area under the ROC curve, or ROC AUC score.\n",
    "\n",
    "# Scores closer to 100% are better. \n",
    "# Scores at 50% or below are very bad.\n",
    "\n",
    "print(f'ROC AUC score on training data is: {roc_auc_score(y_train, y_train_preds)}')\n",
    "print(f'ROC AUC score on testing data is: {}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that our model achieves nearly 69% ROC AUC on the training data, but only about 64% on the testing data. \n",
    "- Since the model was built on the training data, the model has already seen \"the answers.\" Thus, seeing 69% ROC AUC on the training data is probably an overestimate of how our model would perform on \"never-before-seen\" data... since our model had already seen that data during training!\n",
    "- Since the testing data was \"held out,\" we expect that the 64% ROC AUC is a better estimate of how our model would perform on \"never-before-seen\" data. \n",
    "\n",
    "_(Aside: There are a lot of caveats to this! If all of the data you're using for modeling -- whether it's training or testing -- falls prey to some sort of sampling biases, like self-selection bias or survivorship bias or something else, then there's a good chance that your testing ROC AUC [or any metric] is still going to be an overconfident estimate in your model's performance!)_\n",
    "\n",
    "**Our model seems to perform OK here... can we do better?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Motivation behind ensemble modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any one person can make a decision, but that person is susceptible to their own biases and perspectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wisdom of the Crowds\n",
    "<img src=\"./images/penelope.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Inspired by this NPR story](https://www.npr.org/sections/money/2015/08/07/429720443/17-205-people-guessed-the-weight-of-a-cow-heres-how-they-did).\n",
    "\n",
    "1. Gather a group of people. \n",
    "2. Get them each to independently guess Penelope's weight in pounds. (Penelope is the cow in the above image.)\n",
    "    - Someone \"wins\" if their individual guess is closer to the correct answer than the average of all guesses.\n",
    "3. See how many people outperform the average guess!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the players' guesses in this list.\n",
    "list_of_guesses = []\n",
    "\n",
    "# Calculate the average guess.\n",
    "average_guess = np.mean(list_of_guesses)\n",
    "\n",
    "# Create a count of winners, starting at 0.\n",
    "winner_count = 0\n",
    "\n",
    "# Iterate through each of the guesses.\n",
    "for guess in list_of_guesses:\n",
    "    \n",
    "    # If a guess is closer to the true weight (1355 pounds)\n",
    "    # than the average guess:\n",
    "    if abs(guess - 1355) < abs(average_guess - 1355):\n",
    "        \n",
    "        # Add one to our \"winner count\" and print out that \n",
    "        # we have a winner.\n",
    "        winner_count += 1\n",
    "        print(\"We have a winner!\")\n",
    "\n",
    "print(f'The average guess is {average_guess}.')\n",
    "\n",
    "print(f'The number of people who were closer to the truth \\\n",
    "than the average is {winner_count}.')\n",
    "\n",
    "print(f'That is roughly {np.round(winner_count/len(list_of_guesses),0)}% \\\n",
    "of guesses!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "\n",
    "plt.title(f'{winner_count} people were closer to the truth \\\n",
    "than the average guess.',\n",
    "          fontsize=36)\n",
    "\n",
    "plt.hist(list_of_guesses)\n",
    "\n",
    "plt.axvline(1355,\n",
    "            color='orange',\n",
    "            label='Penelope\\'s True Weight')\n",
    "\n",
    "plt.axvline(average_guess,\n",
    "            color='red',\n",
    "            linestyle='--',\n",
    "            label='Average Guess')\n",
    "\n",
    "plt.xlabel('Weight (in pounds)',\n",
    "           fontsize=24)\n",
    "plt.ylabel('Frequency',\n",
    "           rotation=0,\n",
    "           ha='right',\n",
    "           fontsize=24)\n",
    "\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "plt.legend(fontsize=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, a group of individuals will come up with a better guess than one individual on their own. In some cases, an individual may outperform a group of people -- but it is almost always a better bet to go with a group decision than an individual one.\n",
    "\n",
    "##### We call this the wisdom of the crowds. \n",
    "\n",
    "In this way, a model is not terribly different from a person! Individual models, like individual people, will have their own limitations. A model may be too simple for a problem at hand, or too complex. A model may systematically underestimate a quantity of interest, or systematically overestimate it. An individual decision tree will almost surely suffer from some areas where it does not generate good predictions.\n",
    "\n",
    "However, by creating many different models and combining their predictions together in a principled way, we can often improve our overall predictive performance!\n",
    "\n",
    "Just like diverse human perspectives is often correlated with organizations making better decisions and improved business performance, we can apply the same judgment to models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4A: Fit ensemble _bagging_ models with `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among bagging models, we will fit:\n",
    "1. A set of bagged _(bootstrap aggregated)_ decision trees.\n",
    "2. A random forest.\n",
    "3. A set of extremely randomized trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate bagging classifier model.\n",
    "bag = BaggingClassifier(n_estimators=1_000,\n",
    "                        random_state=42)\n",
    "\n",
    "# Fit bagging classifier model to training data.\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "# Generate bagging classifier predictions from training and testing sets.\n",
    "y_train_preds = bag.predict(X_train)\n",
    "y_test_preds = bag.predict(X_test)\n",
    "\n",
    "# Evaluate bagging classifier predictions from training and testing sets.\n",
    "print(f'Bagging classifier model ROC AUC score on training data is: {roc_auc_score(y_train, y_train_preds)}')\n",
    "print(f'Bagging classifier model ROC AUC score on testing data is: {roc_auc_score(y_test, y_test_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest (RF) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate random forest model.\n",
    "rf = RandomForestClassifier(n_estimators=,\n",
    "                            random_state=42)\n",
    "\n",
    "# Fit random forest model to training data.\n",
    "\n",
    "\n",
    "# Generate RF predictions from training and testing sets.\n",
    "y_train_preds =\n",
    "y_test_preds =\n",
    "\n",
    "# Evaluate RF predictions from training and testing sets.\n",
    "print(f'Random forest model ROC AUC score on training data is: {roc_auc_score(y_train, y_train_preds)}')\n",
    "print(f'Random forest model ROC AUC score on testing data is: {}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extremely Randomized Trees (ERT) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Extremely Randomized Trees Classifier.\n",
    "ert = ExtraTreesClassifier(\n",
    "                           random_state=42)\n",
    "\n",
    "# Fit ERT model to training data.\n",
    "\n",
    "\n",
    "# Generate ERT predictions from training and testing sets.\n",
    "y_train_preds = ert.predict(X_train)\n",
    "y_test_preds =\n",
    "\n",
    "# Evaluate ERT predictions from training and testing sets.\n",
    "print(f'Extremely Randomized Trees model ROC AUC score on training data is: {roc_auc_score(y_train, y_train_preds)}')\n",
    "print(f'Extremely Randomized Trees model ROC AUC score on testing data is: {}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4B: Fit ensemble _boosting_ models with `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among boosting models, we will fit:\n",
    "1. An AdaBoost model.\n",
    "2. A Gradient Boosted model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost Classifier (ABC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate AdaBoost Classifier.\n",
    "abc = AdaBoostClassifier(\n",
    "                         random_state=42)\n",
    "\n",
    "# Fit ABC model to training data.\n",
    "\n",
    "\n",
    "# Generate ABC predictions from training and testing sets.\n",
    "y_train_preds =\n",
    "y_test_preds =\n",
    "\n",
    "# Evaluate ABC predictions from training and testing sets.\n",
    "print(f'AdaBoost model ROC AUC score on training data is: {}')\n",
    "print(f'AdaBoost model ROC AUC score on testing data is: {}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Classifier (GBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Gradient Boosting Classifier.\n",
    "gbc\n",
    "\n",
    "# Fit GBC model to training data.\n",
    "\n",
    "\n",
    "# Generate GBC predictions from training and testing sets.\n",
    "\n",
    "\n",
    "# Evaluate GBC predictions from training and testing sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "The most sophisticated model is not guaranteed to be the one with best performance! In this case, one simple decision tree bound at a `max_depth` of 2 has better testing performance than all of the more sophisticated models we ran here.\n",
    "\n",
    "When doing machine learning, there are a lot of **hyperparameters**, or quantities that you must select when building models that cannot be learned from the data. For example, we used 1,000 estimators (e.g. individual decision trees) in each of the ensemble models. _That may not be a good decision!_ \n",
    "- Maybe we want more estimators to improve model performance.\n",
    "- Maybe we want fewer estimators to speed up our model fitting and prediction process.\n",
    "\n",
    "Machine learning models have many, many hyperparameters. \n",
    "- Tree-based models have many defaults, but [you can see lots of the hyperparameters here](https://scikit-learn.org/stable/modules/ensemble.html).\n",
    "- If you're using a $k$-nearest neighbors model, you need to select the value of $k$ (the number of neighbors to take into account when building the model).\n",
    "- If you're using a linear or logistic regression model, you need to select the value of $\\lambda$ (or $\\alpha$ or $C$) that controls how strongly you want to regularize your model during the model fitting process.\n",
    "\n",
    "To sum up:\n",
    "- Fancier or more complex is not necessarily better.\n",
    "- Be sure to explore your hyperparameters! Follow [best practices](https://scikit-learn.org/stable/modules/grid_search.html) when doing so, and it's always, **_always_** a good idea to know what each hyperparameter is doing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
